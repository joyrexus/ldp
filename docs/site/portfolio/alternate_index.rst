.. _alt-research:

****************
Published Papers
****************

**Brain Plasticity: Evidence from children with prenatal brain injury.**
Reilly, J. Levine, S., Nass, R. and Stiles, J.
**Child Neuropsychology**, Eds. J. Reed and J. Warner.  Oxford: Blackwell Publishing, 2008.

**Co-speech gestures influence neural responses in brain regions associated with semantic processing.**
Dick, A. S., Goldin-Meadow, S., Hasson, U., Skipper, J. I., & Small, S. L.
*Human Brain Mapping*, 30.11 (2009), 3509-3526.  (PMID: 19384890).
(:ref:`abstract <alt-hbm-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/hbm_2009.pdf>`__)

**Do parents lead their children by the hand?**
Ozcaliskan, S. & Goldin-Meadow, S.  
*Journal of Child Language*, 32.3 (2005), 481-505.
(:ref:`abstract <alt-jcl-2005>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/jcl_2005.pdf>`__)

**Does language about similarity play a role in fostering similarity comparison in children?**
Ozcaliskan, S., Goldin-Meadow, S., Gentner, D., & Mylander, C.
*Cognition*, 112.2 (2009), 217-228.  (PMID: 19524220).
(:ref:`abstract <alt-cog-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/cog_2009.pdf>`__)

**Does linguistic input play the same role in language learning for children with and without early brain injury?**
Rowe, M.L., Levine, S. C., Fisher, J., & Goldin-Meadow, S.
*Developmental Psychology*, 45.1 (2009), 90-100.  (NIHMS 59552).
(:ref:`abstract <alt-dev-psych-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_psych_2009.pdf>`__)

**Differences in early gesture explain SES disparities in child vocabulary size at school entry.**
Rowe, M.L., & Goldin-Meadow, S.
*Science*, 2009, 323, 951-953.  (NIHMS 89518).
(:ref:`abstract <alt-sci-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/sci_2009.pdf>`__)

**Early gesture predicts language delay in children with pre- or perinatal brain lesions.**
Sauer, E., Levine, S.C., Rowe, M. & Goldin-Meadow, S.
*Child Development*, 81 (2010) 528-539.  (NIHMS 174716).
(:ref:`abstract <alt-chi-dev-2010>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/chi_dev_2010.pdf>`__)

**Early gesture selectively predicts later language learning.**
Rowe, M.L., & Goldin-Meadow, S.
*Developmental Science*, 12.1 (2009), 182-187.  (PMID 19120426).
(:ref:`abstract <alt-dev-sci-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_sci_2009.pdf>`__)

**Emergence of syntax: Commonalities and differences across children.**
Vasilyeva, M., Waterfall, H. & Huttenlocher, J.
*Developmental Science*, 11.1 (2008), 84-97.
(:ref:`abstract <alt-dev-sci-2008>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_sci_2008.pdf>`__)

**Exploring test-retest reliability in fMRI of language: Group and task effects.**
Chen, E. E., & Small, S. L.
*Brain and Language*, 102 (2007), 176-185.  (NIHMS 91756).

**Gesture is at the cutting edge of early language development.**
Ozcaliskan, S. & Goldin-Meadow, S.  
*Cognition*, 96.3 (2005), B101-113.
(:ref:`abstract <alt-cog-2005>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/cog_2005.pdf>`__)

**Gestures orchestrate brain networks for language understanding.**
Skipper, J. I., Goldin-Meadow, S., Nusbaum, H. C., & Small, S. L.
*Current Biology*, 19.8 (2009), 661-667.  (PMID 19327997).
(:ref:`abstract <alt-cbio-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/cbio_2009.pdf>`__)

**How caregiver speech affects children’s language growth.**
Huttenlocher, J., Waterfall, H., Vasilyeva, M., Vevea, J., & Hedges, L.
*Cognitive Psychology*, in press.  (NIHMS 174787).

**Is there an iconic gesture spurt at 26 months?**
Ozcaliskan, S., & Goldin-Meadow, S.
*Integrating gestures: The interdisciplinary nature of gesture*, Eds. G. Stam & M. Ishino.  Amsterdam, NL: John Benjamins, in press.

**Language input and child syntax.**
Huttenlocher, J., Vasilyeva, M., Cymerman, E., & Levine, S.
*Cognitive Psychology*, 45.3 (2002), 337-374.
(:ref:`abstract <alt-cog-psych-2002>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/cog_psych_2002.pdf>`__)

**Learning words by hand: Gesture's role in predicting vocabulary development.**
Rowe, M. L., Ozcaliskan, S., & Goldin-Meadow, S. 
*First Language*, 28.2 (2008), 182-199.
(:ref:`abstract <alt-fir-2008>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/fir_2008.pdf>`__)

**Narrative skill in children with early unilateral brain injury: a limit to functional plasticity.**
Demir, E., Levine, S.C., & Goldin-Meadow, S.
*Developmental Science*, 2010, 13:4, 636-647.
(:ref:`abstract <alt-dev-2010-1>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_2010_1.pdf>`__)

**Neural development of networks for audiovisual speech comprehension.**
Dick, A. S., Solodkin, A., & Small, S. L.
*Brain and Language*, 2010, 114:2, 101-114.  (PMID: 19781755).
(:ref:`abstract <alt-bl-2010>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/bl_2010.pdf>`__)

**Parental goals and talk to children.**
Rowe, M. L., & Casillas, A.
*Infant and Child Development*, in press.
(:ref:`abstract <alt-icd-in-press>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/icd_in_press.pdf>`__)

**Sex differences in language first appear in gesture.**
Ozcaliskan, S., & Goldin-Meadow, S.
*Developmental Science*, 2010, 13:5, 752-760.  (NIHMS 174739).
(:ref:`abstract <alt-dev-2010-2>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_2010_2.pdf>`__)

**When gesture-speech combinations do and do not index linguistic change.**
Ozcaliskan, S. & Goldin-Meadow, S.
*Language and Cognitive Processes*, 2009, 28, 190-217.  (NIHMS 115848).
(:ref:`abstract <alt-lcp-2009>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/lcp_2009.pdf>`__)

**Young children use their hands to tell their mothers what to say.**
Goldin-Meadow, S., Goodrich, W., Sauer, E., & Iverson, J.  
*Developmental Science*, 2007, 10:6, 778-785.
(:ref:`abstract <alt-dev-2007>`, `PDF <http://joy.uchicago.edu/~ldp/static/docs/papers/dev_2007.pdf>`__)

Abstracts
=========

.. _alt-hbm-2009:

Co-speech gestures influence neural responses in brain regions associated with semantic processing
--------------------------------------------------------------------------------------------------

*Dick, A. S., Goldin-Meadow, S., Hasson, U., Skipper, J. I., & Small, S. L. (2009)*

Everyday communication is accompanied by visual information from several
sources, including co-speech gestures, which provide semantic information
listeners use to help disambiguate the speaker’s message. Using fMRI, we
examined how gestures inﬂuence neural activity in brain regions associated with
processing semantic information. The BOLD response was recorded while
participants listened to stories under three audiovisual conditions and one
auditory-only (speech alone) condition. In the ﬁrst audiovisual condition, the
storyteller produced gestures that naturally accompany speech. In the second,
the storyteller made semantically unrelated hand movements. In the third, the
storyteller kept her hands still. In addition to inferior parietal and posterior
superior and middle temporal regions, bilateral posterior superior temporal
sulcus and left anterior inferior frontal gyrus responded more strongly to
speech when it was further accompanied by gesture, regardless of the semantic
relation to speech. However, the right inferior frontal gyrus was sensitive to
the semantic import of the hand movements, demonstrating more activity when hand
movements were semantically unrelated to the accompanying speech. These ﬁndings
show that perceiving hand movements during speech modulates the distributed
pattern of neural activation involved in both biological motion perception and
discourse comprehension, suggesting listeners attempt to ﬁnd meaning, not only
in the words speakers produce, but also in the hand movements that accompany speech.


.. _alt-jcl-2005:

Do parents lead their children by the hand?
-------------------------------------------

*Ozcaliskan, S. & Goldin-Meadow, S. (2005)*

The types of gesture+speech combinations children produce during the early
stages of language development change over time. This change, in turn, predicts
the onset of two-word speech and thus might reflect a cognitive transition that
the child is undergoing. An alternative, however, is that the change merely
reflects changes in the types of gesture+speech combinations that their
caregivers produce. To explore this possibility, we videotaped 40 american
child–caregiver dyads in their homes for 90 minutes when the children were 1;2,
1;6, and 1;10. Each gesture was classified according to type (deictic,
conventional, representational) and the relation it held to speech
(reinforcing, disambiguating, supplementary). Children and their caregivers
produced the same types of gestures and in approximately the same distribution.
However, the children differed from their caregivers in the way they used
gesture in relation to speech. Over time, children produced many more
reinforcing (bike+point at bike), disambiguating (that one+point at bike), and
supplementary combinations (ride+point at bike). In contrast, the frequency and
distribution of caregivers' gesture+speech combinations remained constant over
time. Thus, the changing relation between gesture and speech observed in the
children cannot be traced back to the gestural input the children receive.
Rather, it appears to reflect changes in the children's own skills,
illustrating once again gesture's ability to shed light on developing cognitive
and linguistic processes.

.. _alt-cog-2009:

Does language about similarity play a role in fostering similarity comparison in children?
------------------------------------------------------------------------------------------

*Ozcaliskan, S., Goldin-Meadow, S., Gentner, D., & Mylander, C. (2009)*

Commenting on perceptual similarities between objects stands out as an important
linguistic achievement, one that may pave the way towards noticing and
commenting on more abstract relational commonalities between objects. To explore
whether having a conventional linguistic system is necessary for children to
comment on different types of similarity comparisons, we observed four children
who had not been exposed to usable linguistic input – deaf children whose
hearing losses prevented them from learning spoken language and whose hearing
parents had not exposed them to sign language. These children developed gesture
systems that have language-like structure at many different levels.  Here we ask
whether the deaf children used their gestures to comment on similarity relations
and, if so, which types of relations they expressed. We found that all four deaf
children were able to use their gestures to express similarity comparisons
(point to cat + point to tiger) resembling those conveyed by 40 hearing children
in early gesture + speech combinations (cat + point to tiger). However, the two
groups diverged at later ages. Hearing children, after acquiring the word like,
shifted from primarily expressing global similarity (as in cat/tiger) to
primarily expressing single-property similarity (as in crayon is brown like my
hair). In contrast, the deaf children, lacking an explicit term for similarity,
continued to primarily express global similarity. The ﬁndings underscore the
robustness of similarity comparisons in human communication, but also highlight
the importance of conventional terms for comparison as likely contributors to
routinely expressing more focused similarity relations.



.. _alt-dev-psych-2009:

Does linguistic input play the same role in language learning for children with and without early brain injury?
---------------------------------------------------------------------------------------------------------------

*Rowe, M.L., Levine, S. C., Fisher, J., & Goldin-Meadow, S (2009)*

Children with unilateral pre- or perinatal brain injury (BI) show remarkable
plasticity for language learning. Previous work highlights the important role
that lesion characteristics play in explaining individual variation in
plasticity in the language development of children with BI. The current study
examines whether the linguistic input that children with BI receive from their
caregivers also contributes to this early plasticity, and whether linguistic
input plays a similar role in children with BI as it does in typically
developing (TD) children. Growth in vocabulary and syntactic production is
modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings
indicate that caregiver input is an equally potent predictor of syntactic
growth for children with BI than for TD children. Controlling for input, lesion
characteristics (lesion size, type, seizure history) also affect the language
trajectories of children with BI. Thus, findings illustrate how both
variability in the environment (linguistic input) and variability in the
organism (lesion characteristics) work together to contribute to plasticity in
language learning.

.. _alt-sci-2009:

Differences in early gesture explain SES disparities in child vocabulary size at school entry
---------------------------------------------------------------------------------------------

*Rowe, M. L. & Goldin-Meadow, S. (2009)*

Children from low-socioeconomic status (SES) families, on average, arrive at
school with smaller vocabularies than children from high-SES families. In an
effort to identify precursors to, and possible remedies for, this inequality,
we videotaped 50 children from families with a range of different SES
interacting with parents at 14 months and assessed their vocabulary skills at
54 months. We found that children from high-SES families frequently used
gesture to communicate at 14 months, a relation that was explained by parent
gesture use (with speech controlled). In turn, the fact that children from
high-SES families have large vocabularies at 54 months was explained by the
fact that children from high-SES families have large vocabularies at 54
months was explained by children's gesture use at 14 months. Thus,
differences in early gesture help to explain the disparities in vocabulary
that children bring with them to school.

.. _alt-chi-dev-2010:

Early gesture predicts language delay in children with pre- or perinatal brain lesions
--------------------------------------------------------------------------------------

*Sauer, E., Levine, S.C., Rowe, M. & Goldin-Meadow, S. (2010)*

Does early gesture use predict later productive and receptive vocabulary in
children with pre- or perinatal unilateral brain lesions (PL)? Eleven Children
with PL were categorized into 2 groups based on whether their gesture at 18
months was within or below the range of typically developing (TD) children.
Children with PL whose gesture was within the TD range developed a productive
vocabulary at 22 and 26 months and a receptive vocabulary at 30 months that were
all within the TD range. In contrast, children with PL below the TD range did
not. Gesture was thus an early marker of which children with early unilateral
lesions would eventually experience language delay, suggesting that gesture is a
promising diagnostic tool for persistent delay.

.. _alt-dev-sci-2009:

Early gesture selectively predicts later language learning
----------------------------------------------------------

*Rowe, M. L. & Goldin-Meadow, S. (2009)*

The gestures children produce predict the early stages of spoken language
development. Here we ask whether gesture is a global predictor of language
learning, or whether particular gestures predict particular language outcomes.
We observed 52 children interacting with their caregivers at home, and found
that gesture use at 18 months selectively predicted lexical versus syntactic
skills at 42 months, even with early child speech controlled. Specifically,
number of different meanings conveyed in gesture at 18 months predicted
vocabulary at 42 months, but number of gesture+speech combinations did not. In
contrast, number of gesture+speech combinations, particularly those conveying
sentence-like ideas, produced at 18 months predicted sentence complexity at 42
months, but meanings conveyed in gesture did not. We can thus predict
particular milestones in vocabulary and sentence complexity at age 3 1/2 by
watching how children move their hands two years earlier.

.. _alt-dev-sci-2008:

Emergence of syntax: Commonalities and differences across children
------------------------------------------------------------------

*Vasilyeva, M., Waterfall, H. & Huttenlocher, J. (2008)*

This paper presents the results of a longitudinal examination of syntactic
skills, starting at the age of emergence of simple sentences and continuing
through the emergence of complex sentences. We ask whether there is systematic
variability among children from different socioeconomic backgrounds in the early
stages of sentence production. The results suggest a different answer for simple
versus complex sentences. We found a striking similarity across SES groups on
the measures tapping early mastery of basic syntactic rules of simple sentences.
At the same time, there was a signiﬁcant difference between SES groups in the
mastery of complex sentence structures. This difference emerged at the earliest
stages of production of multi-clause sentences and persisted throughout the
period of observation. The implications of these ﬁndings for the understanding
of mechanisms of syntactic development are discussed.

.. _alt-cog-2005:

Gesture is at the cutting edge of early language development
------------------------------------------------------------

*Ozcaliskan, S. & Goldin-Meadow, S. (2005)*

Children who produce one word at a time often use gesture to supplement their
speech, turning a single word into an utterance that conveys a sentence-like
meaning ('eat'+ point at cookie). Interestingly, the age at which children
first produce supplementary gesture-speech combinations of this sort reliably
predicts the age at which they first produce two-word utterances. Gesture thus
serves as a signal that a child will soon be ready to begin producing
multi-word sentences. The question is what happens next. Gesture could continue
to expand a child's communicative repertoire over development, combining with
words to convey increasingly complex ideas. Alternatively, after serving as an
opening wedge into language, gesture could cease its role as a forerunner of
linguistic change. We addressed this question in a sample of 40 typically
developing children, each observed at 14, 18, and 22 months. The number of
supplementary gesture-speech combinations the children produced increased
significantly from 14 to 22 months. More importantly, the types of
supplementary combinations the children produced changed over time and presaged
changes in their speech. Children produced three distinct constructions across
the two modalities several months before these same constructions appeared
entirely within speech. Gesture thus continues to be at the cutting edge of
early language development, providing stepping-stones to increasingly complex
linguistic constructions.


.. _alt-cbio-2009:

Gestures orchestrate brain networks for language understanding
--------------------------------------------------------------

*Skipper, J. I., Goldin-Meadow, S., Nusbaum, H. C., & Small, S. L. (2009)*

Although the linguistic structure of speech provides valuable communicative
information, nonverbal behaviors can offer additional, often disambiguating
cues. In particular, being able to see the face and hand movements of a speaker
facilitates language comprehension. But how does the brain derive meaningful
information from these movements?  Mouth movements provide information about
phonological aspects of speech. In contrast, cospeech gestures display semantic
information relevant to the intended message. We show that when language
comprehension is accompanied by observable face movements, there is strong
functional connectivity between areas of cortex involved in motor planning and
production and posterior areas thought to mediate phonological aspects of speech
perception. In contrast, language comprehension accompanied by cospeech gestures
is associated with tuning of and strong functional connectivity between motor
planning and production areas and anterior areas thought to mediate semantic
aspects of language comprehension. These areas are not tuned to hand and arm
movements that are not meaningful. Results suggest that when gestures accompany
speech, the motor system works with language comprehension areas to determine
the meaning of those gestures.  Results also suggest that the cortical networks
underlying language comprehension, rather than being ﬁxed, are dynamically
organized by the type of contextual information available to listeners during
face-to-face communication.

.. _alt-cog-psych-2002:

Language input and child syntax
-------------------------------

*Huttenlocher, J., Vasilyeva, M., Cymerman, E., & Levine, S. (2002)*

Existing work on the acquisition of syntax has been concerned mainly with the
early stages of syntactic development. In the present study we examine later
syntactic development in children. Also, existing work has focused on
commonalities in the emergence of syntax. Here we explore individual diﬀerences
among children and their relation to variations in language input. In Study 1 we
ﬁnd substantial individual diﬀerences in childrenÕs mastery of multiclause
sentences and a signiﬁcant rela-tion between those diﬀerences and the proportion
of multiclause sentences in parent speech. We also ﬁnd individual diﬀerences in
the number of noun phrases in children's utterances and a signiﬁcant relation
between those diﬀerences and the number of noun phrases in parent speech. In
Study 2 we ﬁnd greater syntactic growth over a year of preschool in classes
where teachers' speech is more syntactically complex. The implications of our
ﬁndings for the understanding of the sources of syntactic development are
discussed.


.. _alt-fir-2008:

Learning words by hand: Gesture's role in predicting vocabulary development
---------------------------------------------------------------------------

*Rowe, M. L., Ozcaliskan, S., & Goldin-Meadow, S. (2008)*

Children vary widely in how quickly their vocabularies grow. Can looking at
early gesture use in children and parents help us predict this variability? We
videotaped 53 English-speaking parent-child dyads in their homes during their
daily activities for 90-minutes every four months between child age 14 and 34
months. At 42 months, children were given the Peabody Picture Vocabulary Test
(PPVT). We found that child gesture use at 14 months was a significant
predictor of vocabulary size at 42 months, above and beyond the effects of
parent and child word use at 14 months. Parent gesture use at 14 months was not
directly related to vocabulary development, but did relate to child gesture use
at 14 months which, in turn, predicted child vocabulary. These relations hold
even when background factors such as socio-economic status are controlled. The
findings underscore the importance of examining early gesture when predicting
child vocabulary development.

.. _alt-dev-2010-1:

Narrative skill in children with early unilateral brain injury: a limit to functional plasticity
------------------------------------------------------------------------------------------------

*Demir, E., Levine, S.C., & Goldin-Meadow, S. (2010)*

Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for
language learning. Previous work has focused mostly on the emergence of
earlier-developing skills, such as vocabulary and syntax. Here we ask whether
this plasticity for earlier-developing aspects of language extends to more
complex, later-developing language functions by examining the narrative
production of children with PL. Using an elicitation technique that involves
asking children to create stories de novo in response to a story stem, we
collected narratives from 11 children with PL and 20 typically developing (TD)
children. Narratives were analysed for length, diversity of the vocabulary used,
use of complex syntax, complexity of the macro-level narrative structure and use
of narrative evaluation. Children’s language performance on vocabulary and
syntax tasks outside the narrative context was also measured. Findings show that
children with PL produced shorter stories, used less diverse vocabulary,
produced structurally less complex stories at the macro-level, and made fewer
inferences regarding the cognitive states of the story characters. These
differences in the narrative task emerged even though children with PL did not
differ from TD children on vocabulary and syntax tasks outside the narrative
context. Thus, findings suggest that there may be limitations to the plasticity
for language functions displayed by children with PL, and that these limitations
may be most apparent in complex, decontextualized language tasks such as
narrative production.

.. _alt-bl-2010:

Neural development of networks for audiovisual speech comprehension
-------------------------------------------------------------------

*Dick, A. S., Solodkin, A., & Small, S. L. (2010)*

Everyday conversation is both an auditory and a visual phenomenon. While visual
speech information enhances comprehension for the listener, evidence suggests
that the ability to benefit from this information improves with development. A
number of brain regions have been implicated in audiovisual speech
comprehension, but the extent to which the neurobiological substrate in the
child compares to the adult is unknown. In particular, developmental differences
in the network for audiovisual speech comprehension could manifest through the
incorporation of additional brain regions, or through different patterns of
effective connectivity. In the present study we used functional magnetic
resonance imaging and structural equation modeling (SEM) to characterize the
developmental changes in network interactions for audiovisual speech
comprehension. The brain response was recorded while children 8- to 11-years-old
and adults passively listened to stories under audiovisual (AV) and
auditory-only (A) conditions. Results showed that in children and adults, AV
comprehension activated the same fronto-temporo-parietal network of regions
known for their contribution to speech production and perception. However, the
SEM network analysis revealed age-related differences in the functional
interactions among these regions. In particular, the influence of the posterior
inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed
across age groups during AV, but not A speech. This functional pathway might be
important for relating motor and sensory information used by the listener to
identify speech sounds. Further, its development might reflect changes in the
mechanisms that relate visual speech information to articulatory speech
representations through experience producing and perceiving speech.

.. _alt-icd-in-press:

Parental goals and talk to children
-----------------------------------

*Rowe, M. L., & Casillas, A. (in-press)*

Myriad studies support a relation between parental beliefs and behaviors. This
study adds to the literature by focusing on the specific relationship between
parental goals and their communication with toddlers.  Do parents with different
goals talk about different topics with their children?  Parents’ goals for their
30-month-olds were gathered using semi-structured interviews with 47 primary
caregivers, whereas the topics of conversations that took place during interactions
were investigated via coding videotapes of observations in the home.  Parents’ short-
and long-term goals spanned several areas including educational, social-emotional,
developmental and pragmatic goals.  Parental utterances most frequently focused on
pragmatic issues, followed by play and academic topics. Parents who mentioned
long-term educational goals devoted more of their talk to academic topics and less
to pragmatic topics, controlling for socio-economic status.  Thus, parental goals
differ and these differences relate to the conversations parents engage in with their
children.

.. _alt-dev-2010-2:

Sex differences in language first appear in gesture
---------------------------------------------------

*Ozcaliskan, S., & Goldin-Meadow, S. (2010)*

Children differ in how quickly they reach linguistic milestones. Boys typically produce
their first multi-word sentences later than girls do. We ask here whether there are sex
differences in children’s gestures that precede, and presage, these sex differences in
speech. To explore this question, we observed 22 girls and 18 boys every 4 months as they
progressed from one-word speech to multi-word speech. We found that boys not only produced
speech + speech (S+S) combinations (‘drink juice’) 3 months later than girls, but they
also produced gesture + speech (G+S) combinations expressing the same types of semantic
relations (‘eat’ + point at cookie) 3 months later than girls. Because G+S combinations
are produced earlier than S+S combinations, children’s gestures provide the first sign
that boys are likely to lag behind girls in the onset of sentence constructions.

.. _alt-lcp-2009:

When gesture-speech combinations do and do not index linguistic change
----------------------------------------------------------------------

*Ozcaliskan, S. & Goldin-Meadow, S. (2009)*

At the one-word stage children use gesture to supplement their speech
('eat'-point at cookie), and the onset of such supplementary gesture-speech
combinations predicts the onset of two-word speech ('eat cookie'). Gesture does
signal a child's readiness to produce two-word constructions. The question we
ask here is what happens when the child begins to flesh out these early
skeletal two-word constructions with additional arguments. One possibility is
that gesture continues to be a forerunner of linguistic change as children
flesh out their skeletal constructions by adding arguments. Alternatively,
after serving as an opening wedge into language, gesture could cease its role
as a forerunner of linguistic change. Our analysts of 40 children - from 14 to
34 months - showed that children relied on gesture to produce the first
instance of a variety of constructions. However, once each construction was
established in their repertoire, the children did not use gesture to flesh
out the construction. Gesture thus acts as a harbinger of linguistic steps
only when those steps involve new constructions, not when the steps merely
flesh out existing constructions.

.. _alt-dev-2007:

Young children use their hands to tell their mothers what to say
----------------------------------------------------------------

*Goldin-Meadow, S. Goodrich, W., Sauer, E. & Iverson, J. (2007)*

Children produce their first gestures before their first words, and their first
gesture+word sentences before their first word+word sentences. These gestural
accomplishments have been found not only to predate linguistic milestones, but
also to predict them. Findings of this sort suggest that gesture itself might
be playing a role in the language-learning process. But what role does it play?
Children's gestures could elicit from their mothers the kinds of words and
sentences that the children need to hear in order to take their next linguistic
step. We examined maternal responses to the gestures and speech that 10
children produced during the one-word period. We found that all 10 mothers
'translated' their children's gestures into words, providing timely models for
how one- and two-word ideas can be expressed in English. Gesture thus offers a
mechanism by which children can point out their thoughts to mothers, who then
calibrate their speech to those thoughts, and potentially facilitate
language-learning.



